{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whiskycasablanca/CLIP_Implementation/blob/Test_L2/CLIP_Zeroshot_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzjvsJtRLx1D",
        "outputId": "219ee96f-f777-4021-be91-634046abab47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP_Implementation'...\n",
            "remote: Enumerating objects: 13789, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 13789 (delta 2), reused 6 (delta 2), pack-reused 13778 (from 1)\u001b[K\n",
            "Receiving objects: 100% (13789/13789), 512.80 MiB | 17.57 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Updating files: 100% (13780/13780), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b Test_L2 --single-branch https://github.com/whiskycasablanca/CLIP_Implementation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#구글 드라이브에 저장된 best_model.pth 불러오기 위함\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xivkjxR3OJ_B",
        "outputId": "cebc66c7-19d4-4a67-a0c5-1dc11ec60267"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Yq_FvcOa3O",
        "outputId": "998d393a-532d-4684-bc5a-2c21ea0d7ec9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10.jpg\t\t\t\t\t\t resnet50_original.ipynb\n",
            "'11 (1).jpg'\t\t\t\t\t'resnet50 sgd.ipynb'\n",
            " 11.jpg\t\t\t\t\t\t resnet50제출용.ipynb\n",
            " 12.jpg\t\t\t\t\t\t'resnext101 (1).ipynb'\n",
            " 1.jpg\t\t\t\t\t\t resnext101.ipynb\n",
            " 20230721_214259.pdf\t\t\t\t'resnext50 (1).ipynb'\n",
            " 2.jpg\t\t\t\t\t\t resnext50_sgd,논문그대로.ipynb\n",
            "'30^F40.docx'\t\t\t\t\t'resnext car16 drop out .ipynb'\n",
            "'30^F40.gdoc'\t\t\t\t\t'resnext car16 drop out.ipynb'\n",
            " 35A1814F-12BA-4AB9-A4E8-51261F282A8A.jpeg\t resnext논문그대로전처리적게ipynb.ipynb\n",
            " 3.jpg\t\t\t\t\t\t'resnext채널수1 2.ipynb'\n",
            " 4.jpg\t\t\t\t\t\t soccer_Detect_1222.ipynb\n",
            " 5A7DA6F3-F7DE-47DA-9337-778D6C00B7BF.jpeg\t SoccerDetect.ipynb\n",
            " 5.jpg\t\t\t\t\t\t styletransfer_adam.ipynb\n",
            " 6.jpg\t\t\t\t\t\t style_transfer_adam_random_init.ipynb\n",
            " 7.jpg\t\t\t\t\t\t styletransfer_lbfgs.ipynb\n",
            " 8.jpg\t\t\t\t\t\t Untitled\n",
            " 9.jpg\t\t\t\t\t\t Untitled0.ipynb\n",
            " baseline.ipynb\t\t\t\t\t Untitled10.ipynb\n",
            "'baseline_updated (1).ipynb'\t\t\t Untitled1.ipynb\n",
            " best_model0313_10pm.pth\t\t\t Untitled2.ipynb\n",
            " best_model0313.pth\t\t\t\t Untitled3.ipynb\n",
            " best_model.pth\t\t\t\t\t Untitled4.ipynb\n",
            " clip1차결과.ipynb\t\t\t\t Untitled5.ipynb\n",
            " clip_implementation\t\t\t\t Untitled6.ipynb\n",
            " clip_test0313_1.ipynb\t\t\t\t Untitled7.ipynb\n",
            " CLIP_Zeroshot_Test.ipynb\t\t\t Untitled8.ipynb\n",
            "'Colab Notebooks'\t\t\t\t Untitled9.ipynb\n",
            " IMG_6438.MOV\t\t\t\t\t'경통 2주차.docx'\n",
            " PA#4.pdf\t\t\t\t\t'경통 2주차.gdoc'\n",
            "'resnet101 avgpool4.ipynb'\t\t\t'경통 과제.docx'\n",
            " resnet101SGD.ipynb\t\t\t\t'경통 과제.gdoc'\n",
            " resnet101제출용.ipynb\t\t\t\t'박양렬 독후감 스눕.docx'\n",
            " resnet101최종.ipynb\t\t\t\t ㅇㅁㄴㅇㅁㄴㅇㅁㄴㅇㅁㄴ.gdoc\n",
            "'resnet50 Adam lr 0.001 batch 128 ipynb.ipynb'\t ㅇㅁㄴㅇㅁㄴㅇㅁㄴㅇㅁㄴ.txt\n",
            " resnet50adamw전처리많이.ipynb\t\t\t 여름방학_심층심리검사_신청서.hwp\n",
            " resnet50adamw전처리적게.ipynb\t\t\t'인프런 알고리즘 강의'\n",
            " resnet50.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/CLIP_Implementation')"
      ],
      "metadata": {
        "id": "aKysui1fMGyi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/CLIP_Implementation')"
      ],
      "metadata": {
        "id": "KbIUEFYyNNMf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import DistilBertTokenizer\n",
        "from CLIP import CLIPModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "texts = [\n",
        "    \"a photo of a airplane\",\n",
        "    \"a photo of a automobile\",\n",
        "    \"a photo of a bird\",\n",
        "    \"a photo of a cat\",\n",
        "    \"a photo of a deer\",\n",
        "    \"a photo of a dog\",\n",
        "    \"a photo of a frog\",\n",
        "    \"a photo of a horse\",\n",
        "    \"a photo of a ship\",\n",
        "    \"a photo of a truck\"\n",
        "]\n",
        "\n",
        "class ZeroShotImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): 이미지들이 저장된 폴더 경로.\n",
        "            transform (callable, optional): 이미지에 적용할 변환(transform) 함수.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        # 지정된 폴더 내 모든 이미지 파일 경로 읽기\n",
        "        self.image_paths = glob.glob(os.path.join(image_dir, \"*.*\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.transform = transform\n",
        "        self.labels = '/content/CLIP_Implementation/labels.txt'\n",
        "        with open(\"/content/CLIP_Implementation/labels.txt\", \"r\") as f:\n",
        "            self.labels = [int(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "        # 리스트를 PyTorch 텐서로 변환\n",
        "        self.labels_tensor = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터셋에 포함된 이미지 수 반환\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 주어진 인덱스의 이미지 파일 경로 가져오기\n",
        "        img_path = self.image_paths[idx]\n",
        "        # 이미지를 RGB 모드로 열기\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        # transform이 지정되어 있다면 적용\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.labels_tensor[idx]\n",
        "\n",
        "# 예제: Dataset과 DataLoader 사용하기\n",
        "if __name__ == \"__main__\":\n",
        "    # 이미지 전처리 파이프라인 (예: resize, center crop, tensor 변환, 정규화)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),                 # PIL 이미지를 Tensor로 변환 (값 범위: [0, 1])\n",
        "        transforms.Normalize(                  # CLIP의 정규화 mean, std반영\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # 모델 불러오기\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CLIPModel(return_logits=True)\n",
        "    # 저장된 모델 파라미터 로드 (strict=False 옵션으로 누락 키 무시)\n",
        "    model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_model0313.pth\"), strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    dataset = ZeroShotImageDataset(\"/content/CLIP_Implementation/zeroshot_images\", transform=transform)\n",
        "    # DataLoader 생성 (배치 단위로 불러오기)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # \"a photo of [CLS] 토크나이즈\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    tokens = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correction = 0.0\n",
        "        top3_correction = 0.0\n",
        "        total_size = 0.0\n",
        "        # 텍스트 임베딩은 고정해 두고 사용\n",
        "        text_embedding = model.text_encoder(tokens[\"input_ids\"], tokens[\"attention_mask\"])  # shape: (10, proj_dim)\n",
        "        for image, labels, in tqdm(dataloader, desc='Evaluating', unit='batch'):\n",
        "            image, labels = image.to(device), labels.to(device)\n",
        "            # 이미지 임베딩 계산\n",
        "            image_embedding = model.image_encoder(image)  # shape: (batch, proj_dim)\n",
        "\n",
        "            # logit 계산: 이미지 임베딩과 텍스트 임베딩의 내적에 temperature 스케일 적용\n",
        "            logits = image_embedding @ text_embedding.T * torch.exp(model.temperature)\n",
        "            logits = logits\n",
        "\n",
        "            correction += (logits.argmax(dim=1) == labels).float().sum().item()\n",
        "            top3_indices = logits.topk(3, dim=1)[1] # shape: (batch, 3)\n",
        "            top3_correction += (top3_indices == labels.unsqueeze(1)).any(dim=1).float().sum().item()\n",
        "\n",
        "            total_size += labels.size(0)\n",
        "\n",
        "        print(f\"\\nTop-1 Accuracy : {correction / total_size}\")\n",
        "        print(f\"Top-3 Accuracy : {top3_correction / total_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaQhA7OXMpfP",
        "outputId": "448a6f6f-afe8-4b54-bfc6-bc9c5994d96c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 313/313 [01:57<00:00,  2.67batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 Accuracy : 0.5405\n",
            "Top-3 Accuracy : 0.8618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}