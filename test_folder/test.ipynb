{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from text_encoder import TextEncoder\n",
    "from transformers import DistilBertModel, DistilBertTokenizer  # 필요한 임포트 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "texts = [\"This is a sample sentence.\", \"This is another example.\"]\n",
    "inputs= tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device) \n",
    "\n",
    "encoder = TextEncoder(embed_dim=768, proj_dim=256)\n",
    "inputs = encoder(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미지 인코더 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_encoder import ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_encoder=ImageEncoder(3,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101', 'deeplabv3_resnet50', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'fasterrcnn_mobilenet_v3_large_320_fpn', 'fasterrcnn_mobilenet_v3_large_fpn', 'fasterrcnn_resnet50_fpn', 'fasterrcnn_resnet50_fpn_v2', 'fcn_resnet101', 'fcn_resnet50', 'fcos_resnet50_fpn', 'googlenet', 'inception_v3', 'keypointrcnn_resnet50_fpn', 'lraspp_mobilenet_v3_large', 'maskrcnn_resnet50_fpn', 'maskrcnn_resnet50_fpn_v2', 'maxvit_t', 'mc3_18', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'mvit_v1_b', 'mvit_v2_s', 'quantized_googlenet', 'quantized_inception_v3', 'quantized_mobilenet_v2', 'quantized_mobilenet_v3_large', 'quantized_resnet18', 'quantized_resnet50', 'quantized_resnext101_32x8d', 'quantized_resnext101_64x4d', 'quantized_shufflenet_v2_x0_5', 'quantized_shufflenet_v2_x1_0', 'quantized_shufflenet_v2_x1_5', 'quantized_shufflenet_v2_x2_0', 'r2plus1d_18', 'r3d_18', 'raft_large', 'raft_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'retinanet_resnet50_fpn', 'retinanet_resnet50_fpn_v2', 's3d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'ssd300_vgg16', 'ssdlite320_mobilenet_v3_large', 'swin3d_b', 'swin3d_s', 'swin3d_t', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# 사용 가능한 모델 목록 출력\n",
    "print(models.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', \n",
    " 'deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101', 'deeplabv3_resnet50', \n",
    " 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1',\n",
    "   'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'e\n",
    "   fficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', \n",
    "   'fasterrcnn_mobilenet_v3_large_320_fpn', 'fasterrcnn_mobilenet_v3_large_fpn', 'fasterrcnn_resnet50_fpn', \n",
    "   'fasterrcnn_resnet50_fpn_v2', 'fcn_resnet101', 'fcn_resnet50', 'fcos_resnet50_fpn', 'googlenet', 'inception_v3',\n",
    "     'keypointrcnn_resnet50_fpn', 'lraspp_mobilenet_v3_large', 'maskrcnn_resnet50_fpn', 'maskrcnn_resnet50_fpn_v2', \n",
    "     'maxvit_t', 'mc3_18', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', \n",
    "     'mobilenet_v3_small', 'mvit_v1_b', 'mvit_v2_s', 'quantized_googlenet', 'quantized_inception_v3', 'quantized_mobilenet_v2', \n",
    "     'quantized_mobilenet_v3_large', 'quantized_resnet18', 'quantized_resnet50', 'quantized_resnext101_32x8d', 'quantized_resnext101_64x4d', 'quantized_shufflenet_v2_x0_5', 'quantized_shufflenet_v2_x1_0', 'quantized_shufflenet_v2_x1_5', 'quantized_shufflenet_v2_x2_0', 'r2plus1d_18', 'r3d_18', 'raft_large', 'raft_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'retinanet_resnet50_fpn', 'retinanet_resnet50_fpn_v2', 's3d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'ssd300_vgg16', 'ssdlite320_mobilenet_v3_large', 'swin3d_b', 'swin3d_s', 'swin3d_t', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "# 사전 학습된 ViT-B/16 모델 불러오기\n",
    "vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "\n",
    "# 분류 헤드(MLP head)를 Identity로 바꿔서 특징 추출만 하도록 함\n",
    "vit.heads = nn.Identity()\n",
    "\n",
    "# 이제 vit를 이미지 인코더의 base_model로 사용하면,\n",
    "# vit(image)는 classification head 없이 768차원의 feature를 반환합니다.\n",
    "image_encoder = ImageEncoder(base_model=vit, embed_dim=768, proj_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "# 테스트용 이미지 데이터 생성\n",
    "fake_image = torch.rand(1, 3, 224, 224)  # (batch_size=1, channels=3, height=224, width=224)\n",
    "\n",
    "# 모델 실행\n",
    "output = image_encoder(fake_image)\n",
    "\n",
    "# 출력 크기 확인\n",
    "print(output.shape)  # 예상 출력: torch.Size([1, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 모델 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CLIPModel\n",
    "from text_encoder import TextEncoder\n",
    "from transformers import DistilBertModel, DistilBertTokenizer  # 필요한 임포트 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 토크나이저 설정\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "texts = [\"This is a sample sentence.\", \"This is another example.\"]\n",
    "inputs= tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.6918, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = {\n",
    "\"image\" : torch.rand(2, 3, 224, 224).to(device),\n",
    "\"input_ids\" : inputs[\"input_ids\"],\n",
    "\"mask\" : inputs[\"attention_mask\"]\n",
    "}\n",
    "\n",
    "model = CLIPModel().to(device)\n",
    "loss= model(test)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 img_folder='images',\n",
    "                 caption_file= 'captions.txt',\n",
    "                 transform=T.Compose([\n",
    "                           T.Resize((224,224)),\n",
    "                           T.ToTensor(),\n",
    "                           #clip의 정규화 파라미터를 따름\n",
    "                           T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                       std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ]),\n",
    "                 tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased'),\n",
    "                 max_length=40):    #토큰의 90% 분위수=38이므로 40으로 설정함\n",
    "    \n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 1) 캡션 파일 읽어서 {이미지파일명: [캡션1, 캡션2, ...]} 형태로 저장\n",
    "        self.img2captions = {}\n",
    "        #캡션 읽어서 f로 저장\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                #각 라인의 형태: 1026685415_0431cbf574.jpg,A wet black dog is carrying a green toy through the grass\n",
    "                #쉼표 기준으로 분리하기\n",
    "                img_cap, caption_text = line.split(',', 1)\n",
    "                #한 이미지에 5개의 캡션이 대응되므로 value를 리스트로 설정하기\n",
    "                if img_cap not in self.img2captions:\n",
    "                    self.img2captions[img_cap] = []\n",
    "                #key-value 쌍 추가하기\n",
    "                self.img2captions[img_cap].append(caption_text)\n",
    "\n",
    "        # 2) 전체 이미지 파일 리스트 (중복 없이) 생성 ##출력해서 어떻게 생겼는지 확인해볼 필요 있을듯\n",
    "        self.image_files = list(self.img2captions.keys())\n",
    "        \"\"\"사용되는 이유: \n",
    "        인덱싱: getitem(self, idx) 구현 시, self.image_files[idx]와 같이 특정 인덱스의 이미지 파일명을 쉽게 가져올 수 있습니다.\n",
    "        정렬 보장: 딕셔너리는 순서가 보장되지 않기 때문에, 리스트로 변환해두면 인덱스에 따라 일정한 순서로 데이터를 불러올 수 있습니다.\n",
    "\t    DataLoader와의 호환: DataLoader는 len(dataset)과 __getitem__에 기반하여 데이터를 로드합니다. len(dataset)은 보통 self.image_files의 길이로 계산되므로, 이 부분이 있으면 전체 샘플 수를 바로 알 수 있습니다.\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # (1) 이미지 로드\n",
    "        img_name = self.image_files[idx]  # 예) \"1000268201_693b08cb0e.jpg\"\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # (2) 랜덤 캡션 1개 선택\n",
    "        captions = self.img2captions[img_name]  # 5개 중 하나를 뽑음\n",
    "        caption = random.choice(captions)\n",
    "\n",
    "        # (3) 이미지 전처리 => 논문에서 나온 것처럼 resizing만 하면 될듯. 확인해보니 이미지별로 사이즈가 제각각임\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # (4) 텍스트 토큰화\n",
    "        # DistilBertTokenizer 기준 (batch_first=True 형태)\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # text_inputs 출력 예시: {'input_ids': tensor(...), 'attention_mask': tensor(...)}\n",
    "        # 텐서 사이즈가 (1, sequence_length)로 설정되므로 불필요한 첫번째 차원 지우기\n",
    "        for k, v in text_inputs.items():\n",
    "                text_inputs[k] = v.squeeze(0)\n",
    "\n",
    "        return {\n",
    "         \"image\": image,\n",
    "         \"input_ids\": text_inputs[\"input_ids\"],\n",
    "         \"mask\": text_inputs[\"attention_mask\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=Flickr8kDataset()\n",
    "len(a.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.img2captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 문장을 토큰화 했을 떄 몇개의 토큰이 나오는지를 파악하고, 그 분포를 이용해서 max_length를 설정해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 줄의 길이: 228\n",
      "가장 긴 줄 내용: 2354456107_bf5c766a05.jpg,\"An African-American man wearing a green sweatshirt and blue vest is holding up 2 dollar bills in front of his face , while standing on a busy sidewalk in front of a group of men playing instruments .\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"captions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 길이가 가장 긴 문자열(줄)을 찾는다.\n",
    "longest_line = max(lines, key=len)\n",
    "\n",
    "print(\"가장 긴 줄의 길이:\", len(longest_line))\n",
    "print(\"가장 긴 줄 내용:\", longest_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 토큰 길이: 6\n",
      "최대 토큰 길이: 62\n",
      "평균 토큰 길이: 32.155378683013645\n",
      "중앙값 토큰 길이: 32.0\n",
      "80% 분위수: 36.0\n",
      "90% 분위수: 38.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "with open(\"captions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# 각 캡션의 토큰 길이 측정 (예: 최대 512 토큰까지 고려)\n",
    "token_lengths = []\n",
    "for caption in captions:\n",
    "    tokens = tokenizer.encode(caption, truncation=True)\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "print(\"최소 토큰 길이:\", np.min(token_lengths))\n",
    "print(\"최대 토큰 길이:\", np.max(token_lengths))\n",
    "print(\"평균 토큰 길이:\", np.mean(token_lengths))\n",
    "print(\"중앙값 토큰 길이:\", np.median(token_lengths))\n",
    "\n",
    "# 분위수 확인\n",
    "print(\"80% 분위수:\", np.percentile(token_lengths, 80))\n",
    "print(\"90% 분위수:\", np.percentile(token_lengths, 90))\n",
    "\n",
    "#이부분 readme에 적어두자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
