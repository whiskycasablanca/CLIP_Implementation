{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzjvsJtRLx1D",
        "outputId": "9abba9e5-9908-416d-de66-65e535a6b469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP_Implementation'...\n",
            "remote: Enumerating objects: 13786, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 13786 (delta 0), reused 2 (delta 0), pack-reused 13778 (from 1)\u001b[K\n",
            "Receiving objects: 100% (13786/13786), 512.80 MiB | 37.81 MiB/s, done.\n",
            "Updating files: 100% (13780/13780), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b Test_L2 --single-branch https://github.com/whiskycasablanca/CLIP_Implementation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/CLIP_Implementation')"
      ],
      "metadata": {
        "id": "aKysui1fMGyi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/CLIP_Implementation')"
      ],
      "metadata": {
        "id": "KbIUEFYyNNMf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import DistilBertTokenizer\n",
        "from CLIP import CLIPModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "texts = [\n",
        "    \"a photo of a airplane\",\n",
        "    \"a photo of a automobile\",\n",
        "    \"a photo of a bird\",\n",
        "    \"a photo of a cat\",\n",
        "    \"a photo of a deer\",\n",
        "    \"a photo of a dog\",\n",
        "    \"a photo of a frog\",\n",
        "    \"a photo of a horse\",\n",
        "    \"a photo of a ship\",\n",
        "    \"a photo of a truck\"\n",
        "]\n",
        "\n",
        "class ZeroShotImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): 이미지들이 저장된 폴더 경로.\n",
        "            transform (callable, optional): 이미지에 적용할 변환(transform) 함수.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        # 지정된 폴더 내 모든 이미지 파일 경로 읽기\n",
        "        self.image_paths = glob.glob(os.path.join(image_dir, \"*.*\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.transform = transform\n",
        "        self.labels = '/content/CLIP_Implementation/labels.txt'\n",
        "        with open(\"/content/CLIP_Implementation/labels.txt\", \"r\") as f:\n",
        "            self.labels = [int(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "        # 리스트를 PyTorch 텐서로 변환\n",
        "        self.labels_tensor = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터셋에 포함된 이미지 수 반환\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 주어진 인덱스의 이미지 파일 경로 가져오기\n",
        "        img_path = self.image_paths[idx]\n",
        "        # 이미지를 RGB 모드로 열기\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        # transform이 지정되어 있다면 적용\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.labels_tensor[idx]\n",
        "\n",
        "# 예제: Dataset과 DataLoader 사용하기\n",
        "if __name__ == \"__main__\":\n",
        "    # 이미지 전처리 파이프라인 (예: resize, center crop, tensor 변환, 정규화)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),                # 짧은 변을 256으로 리사이즈\n",
        "        transforms.CenterCrop(224),            # 224x224 크기로 중앙 자르기\n",
        "        transforms.ToTensor(),                 # PIL 이미지를 Tensor로 변환 (값 범위: [0, 1])\n",
        "        transforms.Normalize(                  # 정규화 (ImageNet 기준)\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # 모델 불러오기\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CLIPModel(return_logits=True)\n",
        "    # 저장된 모델 파라미터 로드 (strict=False 옵션으로 누락 키 무시)\n",
        "    model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_model.pth\"), strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    dataset = ZeroShotImageDataset(\"/content/CLIP_Implementation/zeroshot_images\", transform=transform)\n",
        "    # DataLoader 생성 (배치 단위로 불러오기)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # \"a photo of [CLS] 토크나이즈\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    tokens = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correction = 0.0\n",
        "        top3_correction = 0.0\n",
        "        total_size = 0.0\n",
        "        # 텍스트 임베딩은 고정해 두고 사용\n",
        "        text_embedding = model.text_encoder(tokens[\"input_ids\"], tokens[\"attention_mask\"])  # shape: (10, proj_dim)\n",
        "        for image, labels, in tqdm(dataloader, desc='Evaluating', unit='batch'):\n",
        "            image, labels = image.to(device), labels.to(device)\n",
        "            # 이미지 임베딩 계산\n",
        "            image_embedding = model.image_encoder(image)  # shape: (batch, proj_dim)\n",
        "\n",
        "            # logit 계산: 이미지 임베딩과 텍스트 임베딩의 내적에 temperature 스케일 적용\n",
        "            logits = image_embedding @ text_embedding.T * torch.exp(model.temperature)\n",
        "            # logits의 shape: (1, num_texts) -> squeeze해서 (num_texts,)로 변환\n",
        "            logits = logits.squeeze(0)\n",
        "\n",
        "            correction += (logits.argmax(dim=1) == labels).float().sum().item()\n",
        "            top3_indices = logits.topk(3, dim=1)[1] # shape: (batch, 3)\n",
        "            top3_correction += (top3_indices == labels.unsqueeze(1)).any(dim=1).float().sum().item()\n",
        "\n",
        "            total_size += labels.size(0)\n",
        "\n",
        "        print(f\"\\nTop-1 Accuracy : {correction / total_size}\")\n",
        "        print(f\"Top-3 Accuracy : {top3_correction / total_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaQhA7OXMpfP",
        "outputId": "4161cfc2-c65b-4c47-b833-9241d315587e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-976a40f3f714>:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_model.pth\"), strict=False)\n",
            "Evaluating: 100%|██████████| 313/313 [02:01<00:00,  2.57batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 Accuracy : 0.4913\n",
            "Top-3 Accuracy : 0.8408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}