#텍스트 인코더 구현=> 텍스트를 인풋으로 받아서 정해진 임베딩 차원(proj_dim)으로 임베딩 만들어줌
#텍스트 인코더의 입력을 만들어줄 토크나이저 구현
#이미지 인코더 구현=>마찬가지로 proj_dim 차원을 가지는 임베딩 만들어준다
#클립 모델구조 만들어주기
#학습을 진행할 train.py가 필요할듯

#CLIP 이미지 정규화 

mean=[0.48145466, 0.4578275, 0.40821073]
std=[0.26862954, 0.26130258, 0.27577711]

#args 파일을 분리하면 좋을듯

# train validation test 데이터셋을 분리하면 좋을듯

#데이터셋 클래스 만들어야할듯

#text_encoder 구현 성공함. 
# pretrained 된 걸로 수정함

#프리트레인된 모델(vit, bert) 통과하면 768차원의 임베딩을 얻을 수 있음
#그거를 각각의 인코더 구조 내에서 self-projection을 이용해서
#256차원의 임베딩으로 변환한다. 

#train.py에서 gpt가 짜준 것처럼 vit불러와서 mlp 헤드 분리하고 인코더 정의할때 써주면 될듯=>models에서 완료함. 필요없음

#image_encoder, text_encoder, models 구현 완벽하게 완료함. test.ipynb를 통해 실험 완료