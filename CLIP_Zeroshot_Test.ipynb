{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whiskycasablanca/CLIP_Implementation/blob/Test_L2/CLIP_Zeroshot_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzjvsJtRLx1D",
        "outputId": "219ee96f-f777-4021-be91-634046abab47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP_Implementation'...\n",
            "remote: Enumerating objects: 13789, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 13789 (delta 2), reused 6 (delta 2), pack-reused 13778 (from 1)\u001b[K\n",
            "Receiving objects: 100% (13789/13789), 512.80 MiB | 17.57 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Updating files: 100% (13780/13780), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b Test_L2 --single-branch https://github.com/whiskycasablanca/CLIP_Implementation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UESYvyeWVb8X",
        "outputId": "464dfbdc-f20e-49fb-cc12-ff8d48b85d2a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mcommit fb23143b30f2a35ba2a9c1e0a7dba498d2164abd\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mTest_L2\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/Test_L2\u001b[m\u001b[33m)\u001b[m\n",
            "Author: Lemon-Farm <fkf00613@gmail.com>\n",
            "Date:   Sat Mar 15 01:42:23 2025 +0900\n",
            "\n",
            "    Moddified Zeroshot_Test.ipynb(Sorting Problem Fixed)\n",
            "\n",
            "\u001b[33mcommit 64958119fbeeacfcdb0da8cafb57651cb4787a51\u001b[m\n",
            "Author: Lemon-Farm <fkf00613@gmail.com>\n",
            "Date:   Sat Mar 15 00:27:39 2025 +0900\n",
            "\n",
            "    Add Zeroshot Test ipynb\n",
            "\n",
            "\u001b[33mcommit 22d52f0143c2d2171d2ae4fdcdcee8ddc9a0e902\u001b[m\n",
            "Author: Lemon-Farm <fkf00613@gmail.com>\n",
            "Date:   Sat Mar 15 00:01:24 2025 +0900\n",
            "\n",
            "    Implemented Zero-Shot Classification Test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#구글 드라이브에 저장된 best_model.pth 불러오기 위함\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xivkjxR3OJ_B",
        "outputId": "cebc66c7-19d4-4a67-a0c5-1dc11ec60267"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/CLIP_Implementation')"
      ],
      "metadata": {
        "id": "aKysui1fMGyi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/CLIP_Implementation')"
      ],
      "metadata": {
        "id": "KbIUEFYyNNMf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import DistilBertTokenizer\n",
        "from CLIP import CLIPModel\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "texts = [\n",
        "    \"a photo of a airplane\",\n",
        "    \"a photo of a automobile\",\n",
        "    \"a photo of a bird\",\n",
        "    \"a photo of a cat\",\n",
        "    \"a photo of a deer\",\n",
        "    \"a photo of a dog\",\n",
        "    \"a photo of a frog\",\n",
        "    \"a photo of a horse\",\n",
        "    \"a photo of a ship\",\n",
        "    \"a photo of a truck\"\n",
        "]\n",
        "\n",
        "class ZeroShotImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): 이미지들이 저장된 폴더 경로.\n",
        "            transform (callable, optional): 이미지에 적용할 변환(transform) 함수.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        # 지정된 폴더 내 모든 이미지 파일 경로 읽기\n",
        "        self.image_paths = glob.glob(os.path.join(image_dir, \"*.*\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.transform = transform\n",
        "        self.labels = '/content/CLIP_Implementation/labels.txt'\n",
        "        with open(\"/content/CLIP_Implementation/labels.txt\", \"r\") as f:\n",
        "            self.labels = [int(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "        # 리스트를 PyTorch 텐서로 변환\n",
        "        self.labels_tensor = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터셋에 포함된 이미지 수 반환\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 주어진 인덱스의 이미지 파일 경로 가져오기\n",
        "        img_path = self.image_paths[idx]\n",
        "        # 이미지를 RGB 모드로 열기\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        # transform이 지정되어 있다면 적용\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.labels_tensor[idx]\n",
        "\n",
        "# 예제: Dataset과 DataLoader 사용하기\n",
        "if __name__ == \"__main__\":\n",
        "    # 이미지 전처리 파이프라인 (예: resize, center crop, tensor 변환, 정규화)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),                 # PIL 이미지를 Tensor로 변환 (값 범위: [0, 1])\n",
        "        transforms.Normalize(                  # CLIP의 정규화 mean, std반영\n",
        "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "            std=[0.26862954, 0.26130258, 0.27577711]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # 모델 불러오기\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CLIPModel(return_logits=True)\n",
        "    # 저장된 모델 파라미터 로드 (strict=False 옵션으로 누락 키 무시)\n",
        "    model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_model0313.pth\"), strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    dataset = ZeroShotImageDataset(\"/content/CLIP_Implementation/zeroshot_images\", transform=transform)\n",
        "    # DataLoader 생성 (배치 단위로 불러오기)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # \"a photo of [CLS] 토크나이즈\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    tokens = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correction = 0.0\n",
        "        top3_correction = 0.0\n",
        "        total_size = 0.0\n",
        "        # 텍스트 임베딩은 고정해 두고 사용\n",
        "        text_embedding = model.text_encoder(tokens[\"input_ids\"], tokens[\"attention_mask\"])  # shape: (10, proj_dim)\n",
        "        for image, labels, in tqdm(dataloader, desc='Evaluating', unit='batch'):\n",
        "            image, labels = image.to(device), labels.to(device)\n",
        "            # 이미지 임베딩 계산\n",
        "            image_embedding = model.image_encoder(image)  # shape: (batch, proj_dim)\n",
        "\n",
        "            # logit 계산: 이미지 임베딩과 텍스트 임베딩의 내적에 temperature 스케일 적용\n",
        "            logits = image_embedding @ text_embedding.T * torch.exp(model.temperature)\n",
        "            logits = logits\n",
        "\n",
        "            correction += (logits.argmax(dim=1) == labels).float().sum().item()\n",
        "            top3_indices = logits.topk(3, dim=1)[1] # shape: (batch, 3)\n",
        "            top3_correction += (top3_indices == labels.unsqueeze(1)).any(dim=1).float().sum().item()\n",
        "\n",
        "            total_size += labels.size(0)\n",
        "\n",
        "        print(f\"\\nTop-1 Accuracy : {correction / total_size}\")\n",
        "        print(f\"Top-3 Accuracy : {top3_correction / total_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaQhA7OXMpfP",
        "outputId": "448a6f6f-afe8-4b54-bfc6-bc9c5994d96c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 313/313 [01:57<00:00,  2.67batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top-1 Accuracy : 0.5405\n",
            "Top-3 Accuracy : 0.8618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}